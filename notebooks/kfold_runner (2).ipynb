{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6ca2257-c5e1-4a98-82fe-83c8d6978243",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../lib/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95097159-59f1-4065-870a-564626d5e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn import model_selection\n",
    "from uuid import uuid4\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import tez\n",
    "\n",
    "from configuration import load_configuration\n",
    "from constants import ColumnNames\n",
    "from image_transforms import (\n",
    "    cnn_training_transform,\n",
    "    cnn_inferencing_transform\n",
    ")\n",
    "\n",
    "from datasets import PetfinderImageSet\n",
    "from models.resnet import PawpularResnetModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d446dd25-e0e2-43cf-bf6e-9c551b1ee8c9",
   "metadata": {},
   "source": [
    "# set up and configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e66d535-d939-461c-a408-24d0761f0109",
   "metadata": {},
   "source": [
    "## Settings not currently in the configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adb74c2f-20b7-4531-ab3d-38dc8bb3434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_KAGLE = False # flag to not if we're in KAGLE and should set paths etc accordingly\n",
    "TRAIN = True # if False will just load and make submission \n",
    "TRAIN &= IS_KAGLE # TRAIN should always be False when running in kaggle\n",
    "\n",
    "LOAD_MODEL = \"pawpular_model_2021_11_08_18\" # only needed when TRAIN is False\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = 11 # how many epocs with no improvment before we stop training. This has reached a point where it should be added to the config soon\n",
    "N_FOLDS = 10 # should probably be added to configuration.yaml at some point\n",
    "USE_HOLDOUT = True # for testing internally we should use a hold out dataset to better measure improvments. but if we are training for submission we should use all the data \n",
    "FEATURE_RESCALLING = False # this is only needed if we add new features, eventually might make it into our config file but for now set here\n",
    "\n",
    "# the scheduler and optimizer are hard one to get into the configuration\n",
    "# these might be things that are specfific to and individual notebook / experiment \n",
    "# so these settings will only exist within notebooks using cosine annealing\n",
    "COSINE_ANNEALING_T0 =5\n",
    "COSINE_ANNEALING_Tmulit =1\n",
    "COSINE_ANNEALING_eta_min = 1e-7\n",
    "\n",
    "configuration_file = \"configuration_exp_20.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244e01e0-a4ff-4cb3-9809-7dc54a3c3922",
   "metadata": {},
   "source": [
    "## Setting the save folder and paths (shouldn't need to change anything here most of the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e3f0817-5d1c-4d53-8a11-8a703d51a05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving in ../experiments/conf_configuration_exp_20_2021_11_20_23\n"
     ]
    }
   ],
   "source": [
    "save_date = datetime.now().strftime(\"%Y_%m_%d_%H\")\n",
    "save_folder = Path('../') / \"experiments\" / f'conf_{configuration_file[:-5]}_{save_date}'\n",
    "save_folder.mkdir(exist_ok=True)\n",
    "print(f\"saving in {save_folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a62cc67f-4480-44e5-b3de-cdef05a2085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_KAGLE:\n",
    "    base_dir = Path('/kaggle')\n",
    "    test_img_dir = base_dir / 'input/petfinder-pawpularity-score/test'\n",
    "    test_data_csv = base_dir / 'input/petfinder-pawpularity-score/test.csv'\n",
    "    train_img_dir = base_dir / 'input/petfinder-pawpularity-score/train'\n",
    "    train_data_csv = base_dir / 'input/petfinder-pawpularity-score/train.csv'\n",
    "    sumbission_csv = base_dir / 'input/petfinder-pawpularity-score/sample_submission.csv'\n",
    "    temp_dir = base_dir / 'temp'\n",
    "    store_dir = base_dir / 'working'\n",
    "    test_submission_path = base_dir/ 'input/pawpularity-submission/submission.csv'\n",
    "    load_dir =  base_dir/ f'pawpular_model_{LOAD_MODEL}'\n",
    "    config = load_configuration(configuration_path = base_dir / f'input/kapet-v003/{configuration_file}')\n",
    "else:\n",
    "    base_dir = Path('../')\n",
    "    test_img_dir = base_dir / 'input/test'\n",
    "    test_data_csv = base_dir / 'input/test.csv'\n",
    "    train_img_dir = base_dir / 'input/train'\n",
    "    train_data_csv = base_dir / 'input/train.csv'\n",
    "    sumbission_csv = base_dir / 'input/sample_submission.csv'\n",
    "    store_dir = base_dir / 'experiments'\n",
    "    test_submission_path = base_dir/ 'input/pawpularity-submission/submission.csv'\n",
    "    load_dir = save_folder\n",
    "    config = load_configuration(configuration_path = base_dir / f'lib/{configuration_file}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3939f2-64ca-4ade-af5a-c43484422aa0",
   "metadata": {},
   "source": [
    "# prepping the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9725554-0630-416f-a6aa-5fee877cce4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files in image train dir 9912\n",
      "total jpgs in image test dir 8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def add_image_path(data,img_dir):\n",
    "\tdata[ColumnNames.image_path.value] = data[ColumnNames.image_name.value].map(lambda x : img_dir / (x + \".jpg\"))\n",
    "\treturn data\n",
    "\n",
    "train_data = pd.read_csv(train_data_csv)\n",
    "test_data = pd.read_csv(test_data_csv)\n",
    "\n",
    "train_data = add_image_path(train_data,train_img_dir)\n",
    "test_data = add_image_path(test_data,test_img_dir)\n",
    "\n",
    "train_data.head()\n",
    "print(f\"total files in image train dir {len(list(train_img_dir.glob('*.*')))}\")\n",
    "print(f\"total jpgs in image test dir {len(list(test_img_dir.glob('*.jpg')))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd18e34-c4e3-4b87-8d5c-e7ce49c8c2b4",
   "metadata": {},
   "source": [
    "## Generate data splits\n",
    "Going to make a hold out data set if specified\n",
    "Ideally if we want to train a final model for submission and we think it has a good set of hyper paramaters we would skip this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "22f3da38-3e26-475b-b2b3-8df71e40cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_data[ColumnNames.label.value]\n",
    "\n",
    "if USE_HOLDOUT:\n",
    "    # x_hold_out and y_hold_out are to be used in as a secondary assecment of fold performance\n",
    "    # different then the validation sets in a fold which are used for early stopping criteria\n",
    "    X, X_hold_out, y, y_hold_out = model_selection.train_test_split(\n",
    "        train_data, y, train_size=0.9, random_state=2019, shuffle=True, stratify=y)\n",
    "else:\n",
    "    X = train_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4986815b-e07b-4c33-82df-2814d7f2bc07",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb4f1e-50c2-4b24-af46-43077137b5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    holdout_losses = []\n",
    "    validation_losses = []\n",
    "    training_losses = []\n",
    "    timing_list = []\n",
    "    k_folds = model_selection.KFold(n_splits=N_FOLDS, random_state=2019, shuffle=True)\n",
    "    for i_fold, (train_index, test_index) in enumerate(k_folds.split(X)):\n",
    "        tic = time.time()\n",
    "        X_train = X.iloc[train_index]\n",
    "        X_validation = X.iloc[test_index]\n",
    "        y_train = y.iloc[train_index]\n",
    "        y_validation = y.iloc[test_index]\n",
    "\n",
    "        # rescale features specific to this fold\n",
    "        if FEATURE_RESCALLING\n",
    "            scaler = StandardScaler()\n",
    "            scaler = scaler.fit(X_train[config.regression_config.features_to_use])\n",
    "            X_train[config.regression_config.features_to_use] = scaler.transform(\n",
    "                X_train[config.regression_config.features_to_use]\n",
    "            )\n",
    "            X_validation[config.regression_config.features_to_use] = scaler.transform(\n",
    "                X_validation[config.regression_config.features_to_use]\n",
    "            )\n",
    "            if USE_HOLDOUT:\n",
    "                X_hold_out[config.regression_config.features_to_use] = scaler.transform(\n",
    "                    X_hold_out[config.regression_config.features_to_use]\n",
    "                )= torch.optim.Adam(model.parameters(), lr=model.learning_rate)\n",
    "\n",
    "        # build data sets\n",
    "        dataset_train = PetfinderImageSet(\n",
    "            config, train_img_dir, images_df=X_train, transform=cnn_training_transform(config)\n",
    "        )\n",
    "        dataset_validation = PetfinderImageSet(\n",
    "            config, train_img_dir, images_df=X_validation, transform=cnn_inferencing_transform(config)\n",
    "        )\n",
    "        \n",
    "        print(f'train size {len(dataset_train)} batches {len(dataset_train) / config.cnn_config.batch_size}')\n",
    "        print(f'validation size {len(dataset_validation)} batches {len(dataset_validation) / config.cnn_config.batch_size}')\n",
    "        if USE_HOLDOUT:\n",
    "            print(f'internal testing size {len(dataset_hold_out)} batches {len(dataset_hold_out) / config.cnn_config.batch_size}')\n",
    "\n",
    "        model = PawpularResnetModel(\n",
    "            config.cnn_config.model_configuration,\n",
    "            learning_rate=config.cnn_config.learning_rate,\n",
    "            number_of_latent_image_features=128,\n",
    "            number_of_additional_features=len(config.regression_config.features_to_use),\n",
    "            number_of_intermediate_regression_variables=64,\n",
    "            regression_dropout=0.2,\n",
    "            model_drop_rate=0.1\n",
    "        )\n",
    "\n",
    "        model.optimizer = torch.optim.Adam(model.parameters(), lr=config.cnn_config.learning_rate)\n",
    "        model.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "                model.optimizer, T_0=COSINE_ANNEALING_T0, T_mult=COSINE_ANNEALING_Tmulit, eta_min=COSINE_ANNEALING_eta_min, last_epoch=-1\n",
    "            ) \n",
    "        \n",
    "\n",
    "        model.step_scheduler_after = \"epoch\"\n",
    "\n",
    "        # early stopping callback, you can also write your own callback\n",
    "        early_stopping = tez.callbacks.EarlyStopping(\n",
    "            monitor=\"valid_loss\",\n",
    "            model_path=save_folder / f\"best_model_fold_{i_fold}.bin\",\n",
    "            patience=EARLY_STOPPING_PATIENCE\n",
    "        )\n",
    "\n",
    "        # train model using the tez framework\n",
    "        model.fit(\n",
    "            train_dataset=dataset_train,\n",
    "            valid_dataset = dataset_validation,\n",
    "            train_bs=8,\n",
    "            valid_bs=8,\n",
    "            device=\"cuda\",\n",
    "            fp16=True,\n",
    "            epochs=150,\n",
    "            callbacks=[early_stopping],\n",
    "            accumulation_steps=2  # needed for memory constraints\n",
    "        )\n",
    "\n",
    "        # save model (with optimizer and scheduler for future!)\n",
    "        model.save(save_folder / f\"final_model_{i_fold}.bin\")\n",
    "        \n",
    "        # reload the best model version\n",
    "        model.load(save_folder / f\"best_model_fold_{i_fold}.bin\")\n",
    "        \n",
    "        # get the best predictions for both validation and train datasets\n",
    "        validation_loss = model.validate_one_epoch(\n",
    "            data_loader=model.valid_loader\n",
    "        )\n",
    "        training_loss = model.validate_one_epoch(\n",
    "            data_loader=model.train_loader\n",
    "        )\n",
    "\n",
    "        # store\n",
    "        validation_losses.append(validation_loss)\n",
    "        training_losses.append(training_loss)\n",
    "        \n",
    "        # run time store\n",
    "        timing = time.time()-tic\n",
    "        timing_list.append(timing)\n",
    "\n",
    "\n",
    "        # some clean up\n",
    "        del dataset_validation\n",
    "        del dataset_train\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # test the holdout scores\n",
    "        if USE_HOLDOUT:\n",
    "            dataset_hold_out = PetfinderImageSet(\n",
    "                config, train_img_dir, images_df=X_hold_out, transform=cnn_inferencing_transform(config)\n",
    "            )\n",
    "            hold_out_loader = torch.utils.data.DataLoader(\n",
    "                dataset_hold_out,\n",
    "                batch_size=8\n",
    "            )\n",
    "            holdout_loss = model.validate_one_epoch(\n",
    "                data_loader=hold_out_loader\n",
    "            )\n",
    "            holdout_losses.append(holdout_loss)\n",
    "            print(f\"Finished fold {i_fold} in {timing} with holdout loss {holdout_loss}\")\n",
    "\n",
    "            del hold_out_loader\n",
    "            del dataset_hold_out\n",
    "        \n",
    "        # even more clean up\n",
    "        del model\n",
    "        gc.collect()\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
